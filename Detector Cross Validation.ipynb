{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from glob import iglob\n",
    "import torch\n",
    "import logging\n",
    "import sys\n",
    "import itertools\n",
    "from shutil import rmtree\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, MultiStepLR\n",
    "\n",
    "from vision.utils.misc import str2bool, Timer, freeze_net_layers, store_labels\n",
    "from vision.ssd.ssd import MatchPrior\n",
    "from vision.ssd.vgg_ssd import create_vgg_ssd\n",
    "from vision.ssd.mobilenetv1_ssd import create_mobilenetv1_ssd\n",
    "from vision.ssd.mobilenetv1_ssd_lite import create_mobilenetv1_ssd_lite\n",
    "from vision.ssd.mobilenet_v2_ssd_lite import create_mobilenetv2_ssd_lite\n",
    "from vision.ssd.squeezenet_ssd_lite import create_squeezenet_ssd_lite\n",
    "from vision.datasets.voc_dataset import VOCDataset\n",
    "from vision.datasets.open_images import OpenImagesDataset\n",
    "from vision.nn.multibox_loss import MultiboxLoss\n",
    "from vision.ssd.config import vgg_ssd_config\n",
    "from vision.ssd.config import mobilenetv1_ssd_config\n",
    "from vision.ssd.config import squeezenet_ssd_config\n",
    "from vision.ssd.data_preprocessing import TrainAugmentation, TestTransform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Arguments:\n",
    "    def __init__(self):\n",
    "        self.dataset_type = 'voc'\n",
    "        self.datasets = None\n",
    "        self.validation_dataset = None\n",
    "        self.balance_data = False\n",
    "        self.net = 'vgg16-ssd'\n",
    "        self.freeze_base_net = False\n",
    "        self.freeze_net = False\n",
    "        self.mb2_width_mult = 1.0\n",
    "        self.learning_rate = 1e-3\n",
    "        self.momentum = 0.9\n",
    "        self.weight_decay = 5e-4\n",
    "        self.gamma = 0.1\n",
    "        self.base_net_lr = None\n",
    "        self.extra_layers_lr = None\n",
    "        self.base_net = None\n",
    "        self.pretrained_ssd = None\n",
    "        self.resume = None\n",
    "        self.scheduler = 'multi-step'\n",
    "        self.milestones = '80,100'\n",
    "        self.t_max = 120\n",
    "        self.batch_size = 32\n",
    "        self.num_epochs = 120\n",
    "        self.num_workers = 4\n",
    "        self.validation_epochs = 5\n",
    "        self.debug_steps = 100\n",
    "        self.use_cuda = True\n",
    "        self.checkpoint_folder = 'models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_dataset_dir = '/home/laps-100/Documentos/dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Arguments()\n",
    "args.net = 'mb1-ssd'\n",
    "args.pretrained_ssd = 'models/mobilenet-v1-ssd-mp-0_675.pth'\n",
    "args.batch_size = 24\n",
    "args.num_epochs = 300\n",
    "args.scheduler = 'cosine'\n",
    "args.lr = 0.01\n",
    "args.base_net_lr = 0.001\n",
    "args.t_max = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_frames_keys(frames_folder):\n",
    "    frames_template = os.path.join(frames_folder, '*.jpg')\n",
    "\n",
    "    frames_keys = []\n",
    "\n",
    "    for frame_path in iglob(frames_template):\n",
    "        basename = os.path.basename(frame_path)\n",
    "        frames_keys.append(os.path.splitext(basename)[0])\n",
    "\n",
    "    np.random.shuffle(frames_keys)\n",
    "    return frames_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cross_dirs(dataset_dir, folds):\n",
    "    JPEGImages_dir = os.path.join(dataset_dir, 'JPEGImages')\n",
    "    Annotations_dir = os.path.join(dataset_dir, 'Annotations')\n",
    "    labels_file = os.path.join(dataset_dir, 'labels.txt')\n",
    "    \n",
    "    frames_keys = np.array(_get_frames_keys(JPEGImages_dir))\n",
    "    \n",
    "    cross_val_dir = os.path.join(os.path.dirname(dataset_dir), 'cross_validation')\n",
    "    if os.path.exists(cross_val_dir):\n",
    "        rmtree(cross_val_dir, ignore_errors=True)\n",
    "        \n",
    "    cross_dirs = []\n",
    "    \n",
    "    kf = KFold(n_splits=folds, shuffle=True)\n",
    "    for i, (train_index, test_index) in enumerate(kf.split(frames_keys)):\n",
    "        dir_name = os.path.join(cross_val_dir, f'fold{i}')\n",
    "        os.makedirs(dir_name)\n",
    "        cross_dirs.append(dir_name)\n",
    "\n",
    "        images_dir = os.path.join(dir_name, 'JPEGImages')\n",
    "        os.symlink(JPEGImages_dir, images_dir, target_is_directory=True)\n",
    "\n",
    "        annotations_dir = os.path.join(dir_name, 'Annotations')\n",
    "        os.symlink(Annotations_dir, annotations_dir, target_is_directory=True)\n",
    "        \n",
    "        tmp_labels_file = os.path.join(dir_name, 'labels.txt')\n",
    "        os.symlink(labels_file, tmp_labels_file)\n",
    "        \n",
    "        sets_dir = os.path.join(dir_name, 'ImageSets', 'Main')\n",
    "        os.makedirs(sets_dir)\n",
    "        \n",
    "        frames_trains, frames_test = frames_keys[train_index], frames_keys[test_index]\n",
    "        \n",
    "        train_file = os.path.join(sets_dir, 'trainval.txt')\n",
    "        with open(train_file, 'w+') as f:\n",
    "            for img_key in frames_trains:\n",
    "                f.write(f'{img_key}\\n')\n",
    "        \n",
    "        test_file = os.path.join(sets_dir, 'test.txt')\n",
    "        with open(test_file, 'w+') as f:\n",
    "            for img_key in frames_test:\n",
    "                f.write(f'{img_key}\\n')\n",
    "        \n",
    "    \n",
    "    return cross_dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    stream=sys.stdout,\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    ")\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(loader, net, criterion, optimizer, device, debug_steps=100, epoch=-1):\n",
    "    net.train(True)\n",
    "    running_loss = 0.0\n",
    "    running_regression_loss = 0.0\n",
    "    running_classification_loss = 0.0\n",
    "    for i, data in enumerate(loader):\n",
    "        images, boxes, labels = data\n",
    "        images = images.to(device)\n",
    "        boxes = boxes.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        confidence, locations = net(images)\n",
    "        regression_loss, classification_loss = criterion(\n",
    "            confidence, locations, labels, boxes\n",
    "        )  # TODO CHANGE BOXES\n",
    "        loss = regression_loss + classification_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        running_regression_loss += regression_loss.item()\n",
    "        running_classification_loss += classification_loss.item()\n",
    "        if i and i % debug_steps == 0:\n",
    "            avg_loss = running_loss / debug_steps\n",
    "            avg_reg_loss = running_regression_loss / debug_steps\n",
    "            avg_clf_loss = running_classification_loss / debug_steps\n",
    "            logging.info(\n",
    "                f\"Epoch: {epoch}, Step: {i}, \"\n",
    "                + f\"Average Loss: {avg_loss:.4f}, \"\n",
    "                + f\"Average Regression Loss {avg_reg_loss:.4f}, \"\n",
    "                + f\"Average Classification Loss: {avg_clf_loss:.4f}\"\n",
    "            )\n",
    "            running_loss = 0.0\n",
    "            running_regression_loss = 0.0\n",
    "            running_classification_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(loader, net, criterion, device):\n",
    "    net.eval()\n",
    "    running_loss = 0.0\n",
    "    running_regression_loss = 0.0\n",
    "    running_classification_loss = 0.0\n",
    "    num = 0\n",
    "    for _, data in enumerate(loader):\n",
    "        images, boxes, labels = data\n",
    "        images = images.to(device)\n",
    "        boxes = boxes.to(device)\n",
    "        labels = labels.to(device)\n",
    "        num += 1\n",
    "\n",
    "        with torch.no_grad():\n",
    "            confidence, locations = net(images)\n",
    "            regression_loss, classification_loss = criterion(\n",
    "                confidence, locations, labels, boxes\n",
    "            )\n",
    "            loss = regression_loss + classification_loss\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        running_regression_loss += regression_loss.item()\n",
    "        running_classification_loss += classification_loss.item()\n",
    "    return (\n",
    "        running_loss / num,\n",
    "        running_regression_loss / num,\n",
    "        running_classification_loss / num,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fold_train(args, iter_i):\n",
    "    timer = Timer()\n",
    "\n",
    "    fold_i = os.path.basename(args.validation_dataset)[4:]\n",
    "    logging.info(f'Starting fold {fold_i} -- iteration {iter_i}')\n",
    "    logging.info(f'Dataset folder: {args.validation_dataset}')\n",
    "    \n",
    "    if args.net == 'vgg16-ssd':\n",
    "        create_net = create_vgg_ssd\n",
    "        config = vgg_ssd_config\n",
    "    elif args.net == 'mb1-ssd':\n",
    "        create_net = create_mobilenetv1_ssd\n",
    "        config = mobilenetv1_ssd_config\n",
    "    elif args.net == 'mb1-ssd-lite':\n",
    "        create_net = create_mobilenetv1_ssd_lite\n",
    "        config = mobilenetv1_ssd_config\n",
    "    elif args.net == 'sq-ssd-lite':\n",
    "        create_net = create_squeezenet_ssd_lite\n",
    "        config = squeezenet_ssd_config\n",
    "    elif args.net == 'mb2-ssd-lite':\n",
    "        create_net = lambda num: create_mobilenetv2_ssd_lite(   # noqa: E731\n",
    "            num, width_mult=args.mb2_width_mult\n",
    "        )\n",
    "        config = mobilenetv1_ssd_config\n",
    "    else:\n",
    "        logging.fatal(\"The net type is wrong.\")\n",
    "        parser.print_help(sys.stderr)\n",
    "        sys.exit(1)\n",
    "    train_transform = TrainAugmentation(\n",
    "        config.image_size, config.image_mean, config.image_std\n",
    "    )\n",
    "    target_transform = MatchPrior(\n",
    "        config.priors, config.center_variance, config.size_variance, 0.5\n",
    "    )\n",
    "\n",
    "    test_transform = TestTransform(\n",
    "        config.image_size, config.image_mean, config.image_std\n",
    "    )\n",
    "\n",
    "    logging.info(\"Prepare training datasets.\")\n",
    "    datasets = []\n",
    "    for dataset_path in args.datasets:\n",
    "        if args.dataset_type == 'voc':\n",
    "            dataset = VOCDataset(\n",
    "                dataset_path,\n",
    "                transform=train_transform,\n",
    "                target_transform=target_transform,\n",
    "            )\n",
    "            label_file = os.path.join(args.checkpoint_folder, \"voc-model-labels.txt\")\n",
    "            store_labels(label_file, dataset.class_names)\n",
    "            num_classes = len(dataset.class_names)\n",
    "        elif args.dataset_type == 'open_images':\n",
    "            dataset = OpenImagesDataset(\n",
    "                dataset_path,\n",
    "                transform=train_transform,\n",
    "                target_transform=target_transform,\n",
    "                dataset_type=\"train\",\n",
    "                balance_data=args.balance_data,\n",
    "            )\n",
    "            label_file = os.path.join(\n",
    "                args.checkpoint_folder, \"open-images-model-labels.txt\"\n",
    "            )\n",
    "            store_labels(label_file, dataset.class_names)\n",
    "            logging.info(dataset)\n",
    "            num_classes = len(dataset.class_names)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Dataset type {args.dataset_type} is not supported.\")\n",
    "        datasets.append(dataset)\n",
    "    logging.info(f\"Stored labels into file {label_file}.\")\n",
    "    train_dataset = ConcatDataset(datasets)\n",
    "    logging.info(\"Train dataset size: {}\".format(len(train_dataset)))\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, args.batch_size, num_workers=args.num_workers, shuffle=True\n",
    "    )\n",
    "    logging.info(\"Prepare Validation datasets.\")\n",
    "    if args.dataset_type == \"voc\":\n",
    "        val_dataset = VOCDataset(\n",
    "            args.validation_dataset,\n",
    "            transform=test_transform,\n",
    "            target_transform=target_transform,\n",
    "            is_test=True,\n",
    "        )\n",
    "    elif args.dataset_type == 'open_images':\n",
    "        val_dataset = OpenImagesDataset(\n",
    "            dataset_path,\n",
    "            transform=test_transform,\n",
    "            target_transform=target_transform,\n",
    "            dataset_type=\"test\",\n",
    "        )\n",
    "        logging.info(val_dataset)\n",
    "    logging.info(\"validation dataset size: {}\".format(len(val_dataset)))\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, args.batch_size, num_workers=args.num_workers, shuffle=False\n",
    "    )\n",
    "    logging.info(\"Build network.\")\n",
    "    net = create_net(num_classes)\n",
    "    min_loss = -10000.0\n",
    "    last_epoch = -1\n",
    "\n",
    "    base_net_lr = args.base_net_lr if args.base_net_lr is not None else args.lr\n",
    "    extra_layers_lr = (\n",
    "        args.extra_layers_lr if args.extra_layers_lr is not None else args.lr\n",
    "    )\n",
    "    if args.freeze_base_net:\n",
    "        logging.info(\"Freeze base net.\")\n",
    "        freeze_net_layers(net.base_net)\n",
    "        params = itertools.chain(\n",
    "            net.source_layer_add_ons.parameters(),\n",
    "            net.extras.parameters(),\n",
    "            net.regression_headers.parameters(),\n",
    "            net.classification_headers.parameters(),\n",
    "        )\n",
    "        params = [\n",
    "            {\n",
    "                'params': itertools.chain(\n",
    "                    net.source_layer_add_ons.parameters(), net.extras.parameters()\n",
    "                ),\n",
    "                'lr': extra_layers_lr,\n",
    "            },\n",
    "            {\n",
    "                'params': itertools.chain(\n",
    "                    net.regression_headers.parameters(),\n",
    "                    net.classification_headers.parameters(),\n",
    "                )\n",
    "            },\n",
    "        ]\n",
    "    elif args.freeze_net:\n",
    "        freeze_net_layers(net.base_net)\n",
    "        freeze_net_layers(net.source_layer_add_ons)\n",
    "        freeze_net_layers(net.extras)\n",
    "        params = itertools.chain(\n",
    "            net.regression_headers.parameters(), net.classification_headers.parameters()\n",
    "        )\n",
    "        logging.info(\"Freeze all the layers except prediction heads.\")\n",
    "    else:\n",
    "        params = [\n",
    "            {'params': net.base_net.parameters(), 'lr': base_net_lr},\n",
    "            {\n",
    "                'params': itertools.chain(\n",
    "                    net.source_layer_add_ons.parameters(), net.extras.parameters()\n",
    "                ),\n",
    "                'lr': extra_layers_lr,\n",
    "            },\n",
    "            {\n",
    "                'params': itertools.chain(\n",
    "                    net.regression_headers.parameters(),\n",
    "                    net.classification_headers.parameters(),\n",
    "                )\n",
    "            },\n",
    "        ]\n",
    "\n",
    "    timer.start(\"Load Model\")\n",
    "    if args.resume:\n",
    "        logging.info(f\"Resume from the model {args.resume}\")\n",
    "        net.load(args.resume)\n",
    "    elif args.base_net:\n",
    "        logging.info(f\"Init from base net {args.base_net}\")\n",
    "        net.init_from_base_net(args.base_net)\n",
    "    elif args.pretrained_ssd:\n",
    "        logging.info(f\"Init from pretrained ssd {args.pretrained_ssd}\")\n",
    "        net.init_from_pretrained_ssd(args.pretrained_ssd)\n",
    "    logging.info(f'Took {timer.end(\"Load Model\"):.2f} seconds to load the model.')\n",
    "\n",
    "    net.to(DEVICE)\n",
    "\n",
    "    criterion = MultiboxLoss(\n",
    "        config.priors,\n",
    "        iou_threshold=0.5,\n",
    "        neg_pos_ratio=3,\n",
    "        center_variance=0.1,\n",
    "        size_variance=0.2,\n",
    "        device=DEVICE,\n",
    "    )\n",
    "    optimizer = torch.optim.SGD(\n",
    "        params, lr=args.lr, momentum=args.momentum, weight_decay=args.weight_decay\n",
    "    )\n",
    "    logging.info(\n",
    "        f\"Learning rate: {args.lr}, Base net learning rate: {base_net_lr}, \"\n",
    "        + f\"Extra Layers learning rate: {extra_layers_lr}.\"\n",
    "    )\n",
    "\n",
    "    if args.scheduler == 'multi-step':\n",
    "        logging.info(\"Uses MultiStepLR scheduler.\")\n",
    "        milestones = [int(v.strip()) for v in args.milestones.split(\",\")]\n",
    "        scheduler = MultiStepLR(\n",
    "            optimizer, milestones=milestones, gamma=0.1, last_epoch=last_epoch\n",
    "        )\n",
    "    elif args.scheduler == 'cosine':\n",
    "        logging.info(\"Uses CosineAnnealingLR scheduler.\")\n",
    "        scheduler = CosineAnnealingLR(optimizer, args.t_max, last_epoch=last_epoch)\n",
    "    else:\n",
    "        logging.fatal(f\"Unsupported Scheduler: {args.scheduler}.\")\n",
    "        parser.print_help(sys.stderr)\n",
    "        sys.exit(1)\n",
    "\n",
    "    logging.info(f\"Start training from epoch {last_epoch + 1}.\")\n",
    "    for epoch in range(last_epoch + 1, args.num_epochs):\n",
    "        scheduler.step()\n",
    "        train(\n",
    "            train_loader,\n",
    "            net,\n",
    "            criterion,\n",
    "            optimizer,\n",
    "            device=DEVICE,\n",
    "            debug_steps=args.debug_steps,\n",
    "            epoch=epoch,\n",
    "        )\n",
    "\n",
    "        if epoch % args.validation_epochs == 0 or epoch == args.num_epochs - 1:\n",
    "            val_loss, val_regression_loss, val_classification_loss = test(\n",
    "                val_loader, net, criterion, DEVICE\n",
    "            )\n",
    "            logging.info(\n",
    "                f\"Epoch: {epoch}, \"\n",
    "                + f\"Validation Loss: {val_loss:.4f}, \"\n",
    "                + f\"Validation Regression Loss {val_regression_loss:.4f}, \"\n",
    "                + f\"Validation Classification Loss: {val_classification_loss:.4f}\"\n",
    "            )\n",
    "    \n",
    "    fold_name = os.path.basename(args.validation_dataset)\n",
    "    \n",
    "    save_path = os.path.join(args.checkpoint_folder, {fold_name})\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "            \n",
    "    model_path = os.path.join(\n",
    "        save_path, f\"{args.net}-{iter_i}-Loss-{val_loss}.pth\"\n",
    "    )\n",
    "            \n",
    "    net.save(model_path)\n",
    "    logging.info(f\"Saved model {model_path}\")\n",
    "\n",
    "    logging.info(f\"Iteration {iter_i} finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\n",
    "    \"cuda:0\" if torch.cuda.is_available() and args.use_cuda else \"cpu\"\n",
    ")\n",
    "\n",
    "if args.use_cuda and torch.cuda.is_available():\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    logging.info(\"Use Cuda.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_dirs = create_cross_dirs(ori_dataset_dir, 5)\n",
    "\n",
    "for fold_i, dataset_folder in enumerate(cross_dirs):\n",
    "    for iter_i in range(10):\n",
    "        args.datasets = [dataset_folder]\n",
    "        args.validation_dataset = dataset_folder\n",
    "\n",
    "        fold_train(args, iter_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dir_fp.cleanup()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
